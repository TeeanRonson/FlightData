{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Rong/anaconda3/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "import statsmodels.formula.api as smf \n",
    "import statsmodels.api as sm\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Question2***\n",
    "\n",
    "Predicting pizza orders\n",
    "\n",
    "Description:\n",
    "Each time a customer calls in to place an order (possibly for her friends), you note the customerâ€™s phone number and know something about the customer based on prior observations and conversations with the delivery person. \n",
    "\n",
    "Observed features:\n",
    "    1.Weight\n",
    "    2.Age\n",
    "    3.Days (since last order)\n",
    "    4.Vegan\n",
    "    5.Cats (if any observed in home)\n",
    "    6.Cash observed in home\n",
    "\n",
    "Target features:\n",
    "    1.Size\n",
    "    2.Toppings\n",
    "\n",
    "Task: There are two separate datasets which have been provided. I will train one or more classifiers on the training data. From there, I will use the trained classifier to generate labels for test data given the features.\n",
    "\n",
    "The notebook will continue as follows: \n",
    "\n",
    "1. Data Observation and cleaning  \n",
    "   - Check for any Nan values in the dataset. If they exist, replace the NaN cell with the most frequently observed value in the respective column using scikit-learn's Imputer. \n",
    "   - Convert String variables to numerical values using scikit-learn's Label Encoder.\n",
    "   - Consolidate dataset\n",
    "   \n",
    "2. Classification task\n",
    "   - K-nearest neighbouurs vs Random Forest\n",
    "   - results \n",
    "   - Conclusion\n",
    "   \n",
    "   \n",
    "The selection of K-nearest Neighbours is due to the simplicity of the algorithm and the potential results it may be able to generate given the context of this dataset. Further, because it is non-parametric, it makes no assumptions about the data and most importantly, it is insensitive to outliers. Although there is a caveat that variables may need to be scaled accordingly to provide reasonable results. This is taken into account in my model. \n",
    "    \n",
    "The selection of Random Forest is due to the robustness of the algorithm in classifying data. Because it is a versatile algorithm and due to the strength of its ensemble learning - encompassing bagging, I believe it will contrast well to K-nearest neighbours. Nonetheless, to ensure it doesnt over fit the data, I have adjusted the arguments in my analysis below. \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***1. Data Observation***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()\n",
    "pizzeria = pd.read_csv('/Users/Rong/Documents/USF/Machine Learning 2/MidTerm2/train.csv')\n",
    "test = pd.read_csv('/Users/Rong/Documents/USF/Machine Learning 2/MidTerm2/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Weight         Age  Days  Vegan  Cats         Cash      Size   Toppings\n",
      "0  106.238809   36.596211    38      0     1     5.699125  No order   No order\n",
      "1  184.378192   28.739952    28      0     0     1.171537  No order   No order\n",
      "2  232.475732  106.605562    38      1     1   259.440103     Large   Hawaiian\n",
      "3  112.811584  103.684648   112      0     0    13.886261  No order   No order\n",
      "4  139.317810   15.045878    78      0     0  1934.054928    Medium  Pepperoni\n",
      "       Weight         Age  Days  Vegan  Cats         Cash\n",
      "0  215.241281   45.123194    19      0     0  1955.034280\n",
      "1  251.301889   17.856168    38      0     0  2532.312093\n",
      "2  189.421541  105.951771     3      0     0   241.320502\n",
      "3   75.000000   37.001579     7      0     0   292.279276\n",
      "4  156.416838   92.159389    63      0     2   325.376085\n"
     ]
    }
   ],
   "source": [
    "pizzeria = pizzeria.iloc[:,1:]\n",
    "test= test.iloc[:,1:]\n",
    "print(pizzeria.head())\n",
    "print(test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Checking for Nan Values***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null values exists? --> False\n",
      "Null values exists? --> False\n"
     ]
    }
   ],
   "source": [
    "print('Null values exists? --> ' +  str(pizzeria.isnull().values.any()))\n",
    "print('Null values exists? --> ' +  str(test.isnull().values.any()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Encode String items into numerical values***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "size_le = LabelEncoder()\n",
    "toppings_le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "size = pizzeria.loc[:,'Size']\n",
    "toppings = pizzeria.loc[:, 'Toppings']\n",
    "\n",
    "size_lbl = size_le.fit_transform(size)\n",
    "toppings_lbl = toppings_le.fit_transform(toppings)\n",
    "\n",
    "pizzeria.loc[:,'Size'] = size_lbl\n",
    "pizzeria.loc[:, 'Toppings'] = toppings_lbl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Curate X and Y Splits***\n",
    "\n",
    "X - contains all the features \n",
    "\n",
    "Y_size - contains the target variable we want to predict for size ordered\n",
    "\n",
    "Y_topping - contains the target variable we want to predict for topping ordered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pizzeria.iloc[:,:-2]\n",
    "Y_size = pizzeria.iloc[:, -2]\n",
    "Y_topping = pizzeria.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***2. Classification Task***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**K-nearest neighbours**\n",
    "\n",
    "Given the context of the question, my first guess would be to attempt solving the prediction using K-nearest neighbours. If we are able to classify individuals based on the features above, I'd suppose that the euclidean distance between one data point and another would allow us to classify the category of size and toppping another data point would fall. \n",
    "\n",
    "Given 500 data points, I opt to sample run the algorithm from 3 neighbours up to 15 neighbours. \n",
    "\n",
    "It is also important to note that scaling of the feature values in necessary since there the variation in the feature values is large. \n",
    "\n",
    "My results dont go beyong a 0.25 and 0.29 for predicting size and toppings respectively. This is somewhat both in absolute amount and relative amounts. \n",
    "\n",
    "\n",
    "Results are shown below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import scale \n",
    "X_scaled = scale(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we use two different classifier here. One for size and the other for toppings. This will be consistent throughout the process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3,16):\n",
    "    clf_size = KNeighborsClassifier(n_neighbors = i)\n",
    "    clf_toppings = KNeighborsClassifier(n_neighbors=i)\n",
    "    clf_size.fit(X_scaled, Y_size)\n",
    "    clf_toppings.fit(X_scaled, Y_topping)\n",
    "    scores_size = cross_val_score(clf_size, X_scaled, Y_size, cv = 10)\n",
    "    scores_topping = cross_val_score(clf_toppings, X_scaled, Y_topping, cv = 10)\n",
    "    print('For k = ' + str(i) + ' neighbours, Mean accuracy for size: %0.4f' % scores_size.mean())\n",
    "    print('For k = ' + str(i) + ' neighbours, Mean accuracy for type: %0.4f' % scores_topping.mean())\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Classifying the test set using K-nearest neighbours***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scaled = scale(test)\n",
    "hypotheses_size = clf_size.predict(test_scaled)\n",
    "hypotheses_toppings = clf_toppings.predict(test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_results = []\n",
    "toppings_results = []\n",
    "for i in hypotheses_size:\n",
    "    size_results.append(size_le.inverse_transform(i))\n",
    "    toppings_results.append(toppings_le.inverse_transform(i))\n",
    "size_results = pd.DataFrame(size_results)\n",
    "toppings_results = pd.DataFrame(toppings_results)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(size_results.head())\n",
    "print(toppings_results.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Random Forest***\n",
    "\n",
    "In pursuit of delivering more robust results, I will use an ensemble method to contrast it agains simplicity of the K-nearest neighbours algorithm.\n",
    "\n",
    "The Random Forest algorithm delivers a significant improvement to the K-nearest neighbours attempt. Almost doubling the accuracy score to a high of 0.51 and 0.57 for size and toppings respectively. Nonetheless, although this is a significant improvement in relative terms, the absolute value is still embarassingly low. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Rong/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('train_accuracy'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Rong/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('train_accuracy'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>train_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.017253</td>\n",
       "      <td>0.001114</td>\n",
       "      <td>0.346154</td>\n",
       "      <td>0.734375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.016627</td>\n",
       "      <td>0.001928</td>\n",
       "      <td>0.490196</td>\n",
       "      <td>0.679287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.001669</td>\n",
       "      <td>0.431373</td>\n",
       "      <td>0.732739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.016791</td>\n",
       "      <td>0.001074</td>\n",
       "      <td>0.450980</td>\n",
       "      <td>0.665924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.017216</td>\n",
       "      <td>0.001216</td>\n",
       "      <td>0.431373</td>\n",
       "      <td>0.710468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.015876</td>\n",
       "      <td>0.000994</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.723831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.013512</td>\n",
       "      <td>0.000796</td>\n",
       "      <td>0.510204</td>\n",
       "      <td>0.674058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.015805</td>\n",
       "      <td>0.000911</td>\n",
       "      <td>0.354167</td>\n",
       "      <td>0.716814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.013279</td>\n",
       "      <td>0.000826</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.727876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.012793</td>\n",
       "      <td>0.000830</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.719027</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fit_time  score_time  test_accuracy  train_accuracy\n",
       "0  0.017253    0.001114       0.346154        0.734375\n",
       "1  0.016627    0.001928       0.490196        0.679287\n",
       "2  0.020000    0.001669       0.431373        0.732739\n",
       "3  0.016791    0.001074       0.450980        0.665924\n",
       "4  0.017216    0.001216       0.431373        0.710468\n",
       "5  0.015876    0.000994       0.333333        0.723831\n",
       "6  0.013512    0.000796       0.510204        0.674058\n",
       "7  0.015805    0.000911       0.354167        0.716814\n",
       "8  0.013279    0.000826       0.333333        0.727876\n",
       "9  0.012793    0.000830       0.437500        0.719027"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_rf_s = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', max_features = 'auto', max_depth = 6, bootstrap = True, random_state = 1)\n",
    "cv_rf_s = cross_validate(clf_rf_s, X=X_scaled, y=Y_size, cv=10, scoring=['accuracy'])\n",
    "#Place into dataframe\n",
    "cv_rf_df = pd.DataFrame(cv_rf_s)\n",
    "cv_rf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Rong/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('train_accuracy'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>train_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.019080</td>\n",
       "      <td>0.001280</td>\n",
       "      <td>0.377358</td>\n",
       "      <td>0.686801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.021321</td>\n",
       "      <td>0.002363</td>\n",
       "      <td>0.415094</td>\n",
       "      <td>0.668904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.023125</td>\n",
       "      <td>0.001351</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.656250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.017935</td>\n",
       "      <td>0.001385</td>\n",
       "      <td>0.480769</td>\n",
       "      <td>0.662946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.016704</td>\n",
       "      <td>0.001173</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.671111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.016011</td>\n",
       "      <td>0.001072</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.625277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.014141</td>\n",
       "      <td>0.000892</td>\n",
       "      <td>0.408163</td>\n",
       "      <td>0.669623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.013386</td>\n",
       "      <td>0.000946</td>\n",
       "      <td>0.479167</td>\n",
       "      <td>0.634956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.013386</td>\n",
       "      <td>0.000882</td>\n",
       "      <td>0.404255</td>\n",
       "      <td>0.629139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.013261</td>\n",
       "      <td>0.000874</td>\n",
       "      <td>0.489362</td>\n",
       "      <td>0.655629</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fit_time  score_time  test_accuracy  train_accuracy\n",
       "0  0.019080    0.001280       0.377358        0.686801\n",
       "1  0.021321    0.002363       0.415094        0.668904\n",
       "2  0.023125    0.001351       0.461538        0.656250\n",
       "3  0.017935    0.001385       0.480769        0.662946\n",
       "4  0.016704    0.001173       0.420000        0.671111\n",
       "5  0.016011    0.001072       0.571429        0.625277\n",
       "6  0.014141    0.000892       0.408163        0.669623\n",
       "7  0.013386    0.000946       0.479167        0.634956\n",
       "8  0.013386    0.000882       0.404255        0.629139\n",
       "9  0.013261    0.000874       0.489362        0.655629"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_rf_t = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', max_features = 'auto', max_depth = 6, bootstrap = True, random_state = 1)\n",
    "cv_rf_t = cross_validate(clf_rf_t, X=X , y=Y_topping, cv=10, scoring=['accuracy'])\n",
    "#Place into dataframe\n",
    "cv_rf_df = pd.DataFrame(cv_rf_t)\n",
    "cv_rf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction results for the test set is shown below, which will also be included as a txt file in the deliverable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_rf_s.fit(X, Y_size)\n",
    "hypotheses = clf_rf_s.predict(test)\n",
    "hypotheses = pd.DataFrame(hypotheses)\n",
    "results_s = pd.DataFrame(size_le.inverse_transform(hypotheses))\n",
    "# print(results_s.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_rf_t.fit(X, Y_topping)\n",
    "hypotheses = clf_rf_t.predict(test)\n",
    "hypotheses = pd.DataFrame(hypotheses)\n",
    "results_t = pd.DataFrame(toppings_le.inverse_transform(hypotheses))\n",
    "# print(results_t.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [results_s, results_t]\n",
    "\n",
    "final_result = pd.concat(frames, axis= 1)\n",
    "final_result.to_csv('MidTerm2PredictII', sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Conclusion***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, the results shown by both selected classifiers for this specific classification task delivers poor results. Further, it is apparent from the txt file, displaying the predicted results for both used algorithms, that the hypothesis predictions for the test data set is far from similar. \n",
    "\n",
    "While recognizing the viability of the data provided, I point to a second path forward: one in which better data are the key to deeper insights. I comment on the lack of information regarding quantity of data that is provided. This will certainly aid the algorithms in making better predictions as limited data may be insufficient to generate/observe patterns in the data. Moreover, the features provided may be of weakness. I believe deeper feature engineering would allow us to manufacture better features to analyse upon. Ultimately aiding our analysis and hence our modelling process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
